{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.services.file_service import load_pdf, split_document\n",
    "from src.services.vectordb_service import ChromaDB\n",
    "from src.utils.util import pretty_print_docs\n",
    "from config import TEMPLATE, MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Use ONLY the following pieces of context to answer the question at the end.\\n'\n",
      " \"If you don't know the answer, just say that you don't know, NEVER make up an \"\n",
      " 'answer.\\n'\n",
      " 'Summarize in bullet point format. Keep the answer as concise as possible.\\n'\n",
      " '{context}\\n'\n",
      " 'Question: {question}\\n'\n",
      " 'Helpful Answer:')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file xLSTM_paper.pdf ...\n",
      "File load correctly. Contains 55 pages\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf()\n",
    "text_splits = split_document(pages, chunk_size=500, chunk_overlap=200, strategy='recursive')\n",
    "chroma = ChromaDB(text_splits)\n",
    "vectorstore = chroma.get_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")\n",
    "#compressed_docs = compression_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(search_type):\n",
    "    if search_type == 'compression':\n",
    "        return compression_retriever\n",
    "    else: \n",
    "        return vectorstore.as_retriever(search_type=search_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the major improvments over the previous LSTM arquitecture?\"\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0)\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(TEMPLATE) # Run chain\n",
    "\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for search_type in ['similarity', 'mmr', 'compression']:\n",
    "    s_type = {}\n",
    "    retreiver = get_retriever(search_type)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retreiver,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    s_type[search_type] = result\n",
    "    search_results.append(s_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "- Replacing every second LSTM layer with a non-gated feed-forward network with GeLU activation function\n",
      "- Adding Exponential Gating to the architecture\n",
      "- Introducing novel memory structures such as scalar memory, scalar update, and memory mixing\n",
      "MMR\n",
      "- xLSTM architecture is remarkably efficient in handling long context problems\n",
      "- xLSTMs with new memory perform best\n",
      "- xLSTM capabilities are boosted by exponential gating and modified memory structures\n",
      "- xLSTMs outperform state-of-the-art Transformers and State Space Models in performance and scaling\n",
      "COMPRESSION\n",
      "- Replacing every second LSTM layer with a non-gated feed-forward network with GeLU activation function\n",
      "- Adding Exponential Gating to the architecture\n",
      "- Introducing novel memory structures\n",
      "- Introducing the new sLSTM and mLSTM variants\n"
     ]
    }
   ],
   "source": [
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        print(search[key]['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observaciones\n",
    "\n",
    "Tanto similarity como compression respondieron un fragmento no del todo correcto para la pregunta.  \n",
    "\"Replacing every second LSTM layer with a non-gated feed-forward network with GeLU activation function\"  \n",
    "\n",
    "Observando el source veo que viene de un fragmento mas grande donde se explica los primeros pasos que se hicieron con la implementacio de LSTM para llegar a la version final.  \n",
    "\n",
    "El tamaño del chunk no le permitio abarcar el contexto donde se ve que es un proceso de primeros cambios y no de la version final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "Document 1 page 1 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "These limitations of LSTM have paved the way for the emergence of Transformers (Vaswani et al.,\n",
      "2017) in language modeling. What performances can we achieve in language modeling when\n",
      "overcoming these limitations and scaling LSTMs to the size of current Large Language Models?\n",
      "2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 40 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "training for LSTMs at this scale. Replacing every second LSTM layer by a non-gated feed-forward\n",
      "network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post\n",
      "up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this\n",
      "architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement.\n",
      "Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 2 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "2 Extended Long Short-Term Memory\n",
      "To overcome the LSTM limitations, Extended Long Short-Term Memory (xLSTM) introduces two\n",
      "main modifications to the LSTM idea of Equation (1). Those modifications – exponential gating\n",
      "and novel memory structures – enrich the LSTM family by two members: (i) the new sLSTM (see\n",
      "Section 2.2) with a scalar memory, a scalar update, and memory mixing, and (ii) the new mLSTM\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "suggesting that the xLSTM architecture is remarkably efficient in handling different aspects of long\n",
      "context problems. For more details, see Appendix B.1.3.\n",
      "4.2 Method Comparison and Ablation Study\n",
      "The main question of this paper is, what can we achieve in language modeling when scaling up the\n",
      "new LSTM variants. Therefore, we train xLSTMs, Transformers, State Space Models, and other\n",
      "methods on 15B tokens from SlimPajama in an auto-regressive language modeling setting. We\n",
      "\n",
      "MMR\n",
      "Document 1 page 1 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "These limitations of LSTM have paved the way for the emergence of Transformers (Vaswani et al.,\n",
      "2017) in language modeling. What performances can we achieve in language modeling when\n",
      "overcoming these limitations and scaling LSTMs to the size of current Large Language Models?\n",
      "2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 35 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "Results. Table 6 shows the result of experiments on the Long Range Arena benchmark. xLSTM\n",
      "demonstrates consistent strong performance on all of the tasks, suggesting that the proposed architec-\n",
      "ture is remarkably efficient in handling different aspects of long context problems.\n",
      "36\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 0 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a\n",
      "matrix memory and a covariance update rule. Integrating these LSTM extensions\n",
      "into residual block backbones yields xLSTM blocks that are then residually stacked\n",
      "into xLSTM architectures. Exponential gating and modified memory structures\n",
      "boost xLSTM capabilities to perform favorably when compared to state-of-the-art\n",
      "Transformers and State Space Models, both in performance and scaling.\n",
      "Memory Cells\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "xLSTMs with new memory (xLSTM[1:0]\n",
      "and xLSTM[7:1]) perform best.Comparing xLSTM to Other Methods. For com-\n",
      "parison, we train models on 15B tokens from SlimPa-\n",
      "jama (Soboleva et al., 2023). The trained models\n",
      "are evaluated by their perplexity on the validation\n",
      "set. We compare the following methods: xLSTM\n",
      "(our new method), GPT-3 (Transformer) (Brown et al.,\n",
      "2020), Llama (Transformer) (Touvron et al., 2023), H3\n",
      "(SSM) (Fu et al., 2023), Mamba (SSM) (Gu & Dao,\n",
      "\n",
      "COMPRESSION\n",
      "Document 1 page 40 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "Replacing every second LSTM layer by a non-gated feed-forward network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement. Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 2 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "- Extended Long Short-Term Memory (xLSTM)\n",
      "- introduces two main modifications to the LSTM idea of Equation (1)\n",
      "- exponential gating and novel memory structures\n",
      "- enrich the LSTM family by two members: (i) the new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing, and (ii) the new mLSTM\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "xLSTM architecture, Appendix B.1.3, new LSTM variants, xLSTMs, Transformers, State Space Models, 15B tokens, SlimPajama, auto-regressive language modeling setting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        pretty_print_docs(search[key]['source_documents'])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
