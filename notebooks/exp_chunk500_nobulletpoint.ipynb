{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file data/raw/xLSTM_paper.pdf ...\n",
      "File load correctly and contains 55 pages \n"
     ]
    }
   ],
   "source": [
    "file_name = \"data/raw/xLSTM_paper.pdf\"\n",
    "print(f'Loading file {file_name} ...')\n",
    "loader = PyPDFLoader(file_name)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f'File load correctly and contains {len(pages)} pages ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document spliting. Chunk size 500 - Chunk overlap 200 - Policy recursive\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 500\n",
    "chunk_overlap = 200\n",
    "splitter_policy = 'recursive'\n",
    "print(f'Document spliting. Chunk size {chunk_size} - Chunk overlap {chunk_overlap} - Policy {splitter_policy}')\n",
    "\n",
    "if (splitter_policy == 'recursive'):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "elif (splitter_policy == 'character'):\n",
    "    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "else:\n",
    "    splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of splits generated 520\n"
     ]
    }
   ],
   "source": [
    "split_texts = splitter.split_documents(pages)\n",
    "print(f'Amount of splits generated {len(split_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a local embedding vector DB on directory data/chroma/\n",
      "DB created successfuly. Collection count: 520 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'data/chroma/'\n",
    "print(f'Creating a local embedding vector DB on directory {persist_directory}')\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_texts,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f'DB created successfuly. Collection count: {vectordb._collection.count()} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1} page {d.metadata['page']} from {d.metadata['source']}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 page 40 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "training for LSTMs at this scale. Replacing every second LSTM layer by a non-gated feed-forward\n",
      "network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post\n",
      "up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this\n",
      "architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement.\n",
      "Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 1 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "These limitations of LSTM have paved the way for the emergence of Transformers (Vaswani et al.,\n",
      "2017) in language modeling. What performances can we achieve in language modeling when\n",
      "overcoming these limitations and scaling LSTMs to the size of current Large Language Models?\n",
      "2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 0 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "billions of parameters, leveraging the latest techniques from modern LLMs, but\n",
      "mitigating known limitations of LSTMs? Firstly, we introduce exponential gating\n",
      "with appropriate normalization and stabilization techniques. Secondly, we modify\n",
      "the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar\n",
      "update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a\n",
      "matrix memory and a covariance update rule. Integrating these LSTM extensions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "performances, evoking an ablation study of the individual new xLSTM components. For doing\n",
      "so, we morph a vanilla LSTM architecture step-by-step into an xLSTM architecture. First, we\n",
      "integrate LSTM layers into pre-LayerNorm residual backbones, second we extend this to a post\n",
      "up-projection block, then we add exponential gating, and finally the matrix memory. The results are\n",
      "shown in Table 2 (top). The ablation studies attribute the strong performance improvement to both\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5 page 2 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "2 Extended Long Short-Term Memory\n",
      "To overcome the LSTM limitations, Extended Long Short-Term Memory (xLSTM) introduces two\n",
      "main modifications to the LSTM idea of Equation (1). Those modifications – exponential gating\n",
      "and novel memory structures – enrich the LSTM family by two members: (i) the new sLSTM (see\n",
      "Section 2.2) with a scalar memory, a scalar update, and memory mixing, and (ii) the new mLSTM\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the major improvments over the previous or vanilla LSTM arquitecture?\"\n",
    "related_docs = vectordb.similarity_search(question,k=5)\n",
    "pretty_print_docs(related_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")\n",
    "compressed_docs = compression_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 page 40 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "Replacing every second LSTM layer by a non-gated feed-forward network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement. Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 0 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "- Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques.\n",
      "- Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "- xLSTM components\n",
      "- vanilla LSTM architecture\n",
      "- pre-LayerNorm residual backbones\n",
      "- post up-projection block\n",
      "- exponential gating\n",
      "- matrix memory\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, NEVER make up an answer. Use three sentences maximum. Keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template) # Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(), #compression_retriever\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando compression en vector retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The major improvements over the previous or vanilla LSTM architecture '\n",
      " 'include replacing every second LSTM layer with a non-gated feed-forward '\n",
      " 'network with GeLU activation function, adding Exponential Gating to this '\n",
      " 'architecture, and integrating these LSTM extensions such as sLSTM and mLSTM. '\n",
      " 'These improvements also involve modifying the LSTM memory structure, using '\n",
      " 'pre-LayerNorm residual backbones, and incorporating a post up-projection '\n",
      " 'block and a matrix memory.')\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the major improvments over the previous or vanilla LSTM arquitecture?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "pprint.pp(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 page 40 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "Replacing every second LSTM layer by a non-gated feed-forward network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement. Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 0 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "- exponential gating with appropriate normalization and stabilization techniques\n",
      "- modify the LSTM memory structure\n",
      "- sLSTM with a scalar memory, a scalar update, and new memory mixing\n",
      "- mLSTM that is fully parallelizable with a matrix memory and a covariance update rule\n",
      "- integrating these LSTM extensions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "- xLSTM components\n",
      "- vanilla LSTM architecture\n",
      "- pre-LayerNorm residual backbones\n",
      "- post up-projection block\n",
      "- exponential gating\n",
      "- matrix memory\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sin utilizar compression en vector retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The major improvements over the previous or vanilla LSTM architecture '\n",
      " 'include the introduction of exponential gating with appropriate '\n",
      " 'normalization and stabilization techniques, modification of the LSTM memory '\n",
      " 'structure to obtain sLSTM and mLSTM variants, and integration of these LSTM '\n",
      " 'extensions into the xLSTM architecture. These improvements have been shown '\n",
      " 'to significantly boost performance in language modeling tasks.')\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the major improvments over the previous or vanilla LSTM arquitecture?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "pprint.pp(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 page 1 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "These limitations of LSTM have paved the way for the emergence of Transformers (Vaswani et al.,\n",
      "2017) in language modeling. What performances can we achieve in language modeling when\n",
      "overcoming these limitations and scaling LSTMs to the size of current Large Language Models?\n",
      "2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 40 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "training for LSTMs at this scale. Replacing every second LSTM layer by a non-gated feed-forward\n",
      "network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post\n",
      "up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this\n",
      "architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement.\n",
      "Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 0 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "billions of parameters, leveraging the latest techniques from modern LLMs, but\n",
      "mitigating known limitations of LSTMs? Firstly, we introduce exponential gating\n",
      "with appropriate normalization and stabilization techniques. Secondly, we modify\n",
      "the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar\n",
      "update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a\n",
      "matrix memory and a covariance update rule. Integrating these LSTM extensions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "Ablation Studies. Table 1 and Figure 6 demonstrate that xLSTM achieves excellent results at\n",
      "language modeling when being trained on 15B tokens from SlimPajama. Thus, it is only natural\n",
      "to ask which of the elements of xLSTM is responsible for the improvements over vanilla LSTM\n",
      "performances, evoking an ablation study of the individual new xLSTM components. For doing\n",
      "so, we morph a vanilla LSTM architecture step-by-step into an xLSTM architecture. First, we\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambiando la extructura de la pregunta se obtuvieron mejores resultados (compression_retriever) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Introduction of exponential gating with normalization and stabilization techniques\n",
      "- Modification of LSTM memory structure to include sLSTM with scalar memory and update, and mLSTM with matrix memory and covariance update rule\n",
      "- These extensions allow for better handling of long context problems and improved efficiency in language modeling.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the major improvments over the previous LSTM arquitecture summarize in bullet points?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambiando la extructura de la pregunta se obtuvieron mejores resultados (similarity retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the major improvments over the previous LSTM arquitecture summarize in bullet points?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Exponential gating with normalization and stabilization techniques\n",
      "- Modification of LSTM memory structure, resulting in sLSTM and mLSTM variants\n",
      "- Fully parallelizable mLSTM with matrix memory and covariance update rule\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
