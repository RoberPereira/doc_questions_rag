{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run locally and access project folder\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "base_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.services.file_service import load_pdf, split_document\n",
    "from src.services.vectordb_service import ChromaDB\n",
    "from src.utils.util import pretty_print_docs\n",
    "from config import TEMPLATE, MODEL, DOC_PATH\n",
    "DOC_PATH = base_dir+'/'+DOC_PATH+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Use ONLY the following pieces of context to answer the question at the end.\\n'\n",
      " \"If you don't know the answer, just say that you don't know, NEVER make up an \"\n",
      " 'answer.\\n'\n",
      " 'Summarize in bullet point format. Keep the answer as concise as possible.\\n'\n",
      " '{context}\\n'\n",
      " 'Question: {question}\\n'\n",
      " 'Helpful Answer:')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file xLSTM_paper.pdf ...\n",
      "File load correctly. Contains 55 pages\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf(doc_path=DOC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document spliting. Chunk size 500 - Chunk overlap 200 - Strategy recursive\n",
      "Splits generated 520\n",
      "Cleaning Chroma DB\n",
      "Creating a local embedding vector DB on directory data/chroma\n",
      "DB created successfuly. Collection count: 520 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_splits = split_document(pages, chunk_size=500, chunk_overlap=200, strategy='recursive')\n",
    "chroma = ChromaDB(text_splits)\n",
    "vectorstore = chroma.get_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")\n",
    "#compressed_docs = compression_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(search_type):\n",
    "    if search_type == 'compression':\n",
    "        return compression_retriever\n",
    "    else: \n",
    "        return vectorstore.as_retriever(search_type=search_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the major improvments over the previous or vanilla LSTM arquitecture?\"\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0)\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for search_type in ['similarity', 'mmr', 'compression']:\n",
    "    s_type = {}\n",
    "    retreiver = vectorstore.as_retriever(search_type=search_type)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retreiver,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    s_type[search_type] = result\n",
    "    search_results.append(s_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando compression en vector retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The major improvements over the previous or vanilla LSTM architecture '\n",
      " 'include replacing every second LSTM layer with a non-gated feed-forward '\n",
      " 'network with GeLU activation function, adding Exponential Gating to this '\n",
      " 'architecture, and integrating these LSTM extensions such as sLSTM and mLSTM. '\n",
      " 'These improvements also involve modifying the LSTM memory structure, using '\n",
      " 'pre-LayerNorm residual backbones, and incorporating a post up-projection '\n",
      " 'block and a matrix memory.')\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the major improvments over the previous or vanilla LSTM arquitecture?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "pprint.pp(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 page 40 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "Replacing every second LSTM layer by a non-gated feed-forward network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement. Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 0 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "- exponential gating with appropriate normalization and stabilization techniques\n",
      "- modify the LSTM memory structure\n",
      "- sLSTM with a scalar memory, a scalar update, and new memory mixing\n",
      "- mLSTM that is fully parallelizable with a matrix memory and a covariance update rule\n",
      "- integrating these LSTM extensions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "- xLSTM components\n",
      "- vanilla LSTM architecture\n",
      "- pre-LayerNorm residual backbones\n",
      "- post up-projection block\n",
      "- exponential gating\n",
      "- matrix memory\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sin utilizar compression en vector retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The major improvements over the previous or vanilla LSTM architecture '\n",
      " 'include the introduction of exponential gating with appropriate '\n",
      " 'normalization and stabilization techniques, modification of the LSTM memory '\n",
      " 'structure to obtain sLSTM and mLSTM variants, and integration of these LSTM '\n",
      " 'extensions into the xLSTM architecture. These improvements have been shown '\n",
      " 'to significantly boost performance in language modeling tasks.')\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the major improvments over the previous or vanilla LSTM arquitecture?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "pprint.pp(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 page 1 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "These limitations of LSTM have paved the way for the emergence of Transformers (Vaswani et al.,\n",
      "2017) in language modeling. What performances can we achieve in language modeling when\n",
      "overcoming these limitations and scaling LSTMs to the size of current Large Language Models?\n",
      "2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 40 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "training for LSTMs at this scale. Replacing every second LSTM layer by a non-gated feed-forward\n",
      "network with GeLU activation function (similar to Vaswani et al.), which corresponds to the post\n",
      "up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this\n",
      "architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement.\n",
      "Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 0 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "billions of parameters, leveraging the latest techniques from modern LLMs, but\n",
      "mitigating known limitations of LSTMs? Firstly, we introduce exponential gating\n",
      "with appropriate normalization and stabilization techniques. Secondly, we modify\n",
      "the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar\n",
      "update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a\n",
      "matrix memory and a covariance update rule. Integrating these LSTM extensions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "Ablation Studies. Table 1 and Figure 6 demonstrate that xLSTM achieves excellent results at\n",
      "language modeling when being trained on 15B tokens from SlimPajama. Thus, it is only natural\n",
      "to ask which of the elements of xLSTM is responsible for the improvements over vanilla LSTM\n",
      "performances, evoking an ablation study of the individual new xLSTM components. For doing\n",
      "so, we morph a vanilla LSTM architecture step-by-step into an xLSTM architecture. First, we\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambiando la extructura de la pregunta se obtuvieron mejores resultados (compression_retriever) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Introduction of exponential gating with normalization and stabilization techniques\n",
      "- Modification of LSTM memory structure to include sLSTM with scalar memory and update, and mLSTM with matrix memory and covariance update rule\n",
      "- These extensions allow for better handling of long context problems and improved efficiency in language modeling.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the major improvments over the previous LSTM arquitecture summarize in bullet points?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambiando la extructura de la pregunta se obtuvieron mejores resultados (similarity retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the major improvments over the previous LSTM arquitecture summarize in bullet points?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Exponential gating with normalization and stabilization techniques\n",
      "- Modification of LSTM memory structure, resulting in sLSTM and mLSTM variants\n",
      "- Fully parallelizable mLSTM with matrix memory and covariance update rule\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
