{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run locally and access project folder\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "base_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.services.file_service import load_pdf, split_document\n",
    "from src.services.vectordb_service import ChromaDB\n",
    "from src.utils.util import pretty_print_docs\n",
    "from config import TEMPLATE, MODEL, DOC_PATH\n",
    "DOC_PATH = base_dir+'/'+DOC_PATH+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Use ONLY the following pieces of context to answer the question at the end.\\n'\n",
      " \"If you don't know the answer, just say that you don't know, NEVER make up an \"\n",
      " 'answer.\\n'\n",
      " 'Summarize in bullet point format. Keep the answer as concise as possible.\\n'\n",
      " '{context}\\n'\n",
      " 'Question: {question}\\n'\n",
      " 'Helpful Answer:')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file xLSTM_paper.pdf ...\n",
      "File load correctly. Contains 55 pages\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf(doc_path=DOC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document spliting. Chunk size 800 - Chunk overlap 400 - Strategy recursive\n",
      "Splits generated 364\n",
      "Creating a local embedding vector DB on directory data/chroma\n",
      "DB created successfuly. Collection count: 364 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_splits = split_document(pages, chunk_size=800, chunk_overlap=400, strategy='recursive')\n",
    "chroma = ChromaDB(text_splits)\n",
    "vectorstore = chroma.get_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")\n",
    "#compressed_docs = compression_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(search_type):\n",
    "    if search_type == 'compression':\n",
    "        return compression_retriever\n",
    "    else: \n",
    "        return vectorstore.as_retriever(search_type=search_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the major improvments over the previous LSTM arquitecture?\"\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0)\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for search_type in ['similarity', 'mmr', 'compression']:\n",
    "    s_type = {}\n",
    "    retreiver = get_retriever(search_type)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retreiver,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    s_type[search_type] = result\n",
    "    search_results.append(s_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "- Exponential gating and novel memory structures\n",
      "- Introduction of sLSTM with scalar memory, scalar update, and memory mixing\n",
      "- Introduction of mLSTM with matrix memory and covariance update rule\n",
      "- Fully parallelizable architecture\n",
      "- Linear computation and constant memory complexity with respect to sequence length\n",
      "- Compressive memory well suited for industrial applications and edge implementations\n",
      "MMR\n",
      "- Introduction of exponential gating with appropriate normalization and stabilization techniques\n",
      "- Modification of LSTM memory structure to include sLSTM with scalar memory, scalar update, and new memory mixing, and mLSTM with matrix memory and covariance update rule\n",
      "- Integration of these LSTM extensions into residual block backbones to create xLSTM blocks and architectures\n",
      "COMPRESSION\n",
      "The major improvements over the previous LSTM architecture are the introduction of exponential gating and novel memory structures, including the sLSTM with scalar memory and update, and the mLSTM with matrix memory and covariance update rule.\n"
     ]
    }
   ],
   "source": [
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        print(search[key]['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observaciones\n",
    "\n",
    "La respuesta dada por Similarity search la entiendo como la mas correcta y completa de todas las generadas. A su vez deja de figurar como el resultado erroneo visto en chunks de 500.  \n",
    "La arquitectura no es Fully parallelizable, unicamente el bloque mLSTM lo es. Pero el chunk tiene la informacion correcta, pienso que la pregunta al declarar el nombre de la nueva arquitectura no distingue entre xLSTM - mLSTM - sLSTM.\n",
    "\n",
    "El metodo de Compression por alguna razon ignora la indicacion de responder en bullet points. A su vez si bien la respuesta no es incorrecta, parecen faltar detalles importantes como la arquitectura parallelizable y el orden de ejecucion lineal con respecto a la entrada. Por lo que comprimir el contexto si se busca tener mayor detalles no parece ser la mejor opcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "Document 1 page 0 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a\n",
      "simple question: How far do we get in language modeling when scaling LSTMs to\n",
      "billions of parameters, leveraging the latest techniques from modern LLMs, but\n",
      "mitigating known limitations of LSTMs? Firstly, we introduce exponential gating\n",
      "with appropriate normalization and stabilization techniques. Secondly, we modify\n",
      "the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar\n",
      "update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a\n",
      "matrix memory and a covariance update rule. Integrating these LSTM extensions\n",
      "into residual block backbones yields xLSTM blocks that are then residually stacked\n",
      "into xLSTM architectures. Exponential gating and modified memory structures\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 2 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "2 Extended Long Short-Term Memory\n",
      "To overcome the LSTM limitations, Extended Long Short-Term Memory (xLSTM) introduces two\n",
      "main modifications to the LSTM idea of Equation (1). Those modifications – exponential gating\n",
      "and novel memory structures – enrich the LSTM family by two members: (i) the new sLSTM (see\n",
      "Section 2.2) with a scalar memory, a scalar update, and memory mixing, and (ii) the new mLSTM\n",
      "(see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully\n",
      "parallelizable. Both sLSTM and mLSTM enhance the LSTM through exponential gating. To enable\n",
      "parallelization, the mLSTM abandons memory mixing, i.e., the hidden-hidden recurrent connections.\n",
      "Both mLSTM and sLSTM can be extended to multiple memory cells, where sLSTM features memory\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 40 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "up-projection backbone (see Figure 3) further boosts performance. Adding Exponential Gating to this\n",
      "architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement.\n",
      "Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks\n",
      "with the mLSTM (see Figure 10) gives xLSTM[7:1] with the best performance.\n",
      "Details on Gating Technique Ablation Study. In Table 2 (bottom), we investigate the effect\n",
      "of trainable and input-dependent gates for mLSTM. The results show that, in contrast to other\n",
      "methods (Katharopoulos et al., 2020; Sun et al., 2023; Qin et al., 2023; Katsch, 2023; Yang et al.,\n",
      "2023; Qin et al., 2024; Peng et al., 2024), having the gates both learnable and input dependent gives\n",
      "the best results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 5 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "xLSTM Architecture. An xLSTM architecture is constructed by residually stacking build-\n",
      "ing blocks (Srivastava et al., 2015; He et al., 2016). We rely on the most commonly used pre-\n",
      "LayerNorm (Ba et al., 2016b) residual backbones as used in contemporary Large Language Models.\n",
      "See last column in Figure 1.\n",
      "2.5 Memory and Speed Considerations\n",
      "Contrary to Transformers, xLSTM networks have a linear computation and a constant memory\n",
      "complexity with respect to the sequence length. Since the xLSTM memory is compressive, it is well\n",
      "suited for industrial applications and implementations on the edge.\n",
      "The memory of mLSTM does not require parameters but is computationally expensive through its d×d\n",
      "matrix memory and d×dupdate. We trade off memory capacity against computational complexity.\n",
      "\n",
      "MMR\n",
      "Document 1 page 0 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a\n",
      "simple question: How far do we get in language modeling when scaling LSTMs to\n",
      "billions of parameters, leveraging the latest techniques from modern LLMs, but\n",
      "mitigating known limitations of LSTMs? Firstly, we introduce exponential gating\n",
      "with appropriate normalization and stabilization techniques. Secondly, we modify\n",
      "the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar\n",
      "update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a\n",
      "matrix memory and a covariance update rule. Integrating these LSTM extensions\n",
      "into residual block backbones yields xLSTM blocks that are then residually stacked\n",
      "into xLSTM architectures. Exponential gating and modified memory structures\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 35 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "Experiment Setup. The architectures that are tested in this experiment comprise LLama, Mamba,\n",
      "LSTM, RWKV-4, and xLSTM. LSTM (Block) refers to an architecture where a vanilla LSTM is used\n",
      "inside a post up-projection block (like Transformer with attention replaced by LSTM). For xLSTM\n",
      "we choose the best performing of xLSTM[0:1] or xLSTM[1:0] on the validation set, specifically the\n",
      "former for the Image tasks and the latter for all other ones.\n",
      "We use the hyperparameter settings of the S5 model (Smith et al., 2022) and Linear Recurrent Unit\n",
      "model (Orvieto et al., 2023), with additional hyperparamter search on learning rates and schedulers\n",
      "for all models. We use two different schedulers: Linear Warm-up Cosine Annealing and Linear\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 5 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "et al., 2022), S5 model (Smith et al., 2022), Bidirectional Gated SSM (BiGS) (Wang et al., 2022), H3\n",
      "model (Fu et al., 2023), and Mamba (Gu & Dao, 2023).\n",
      "Recurrent Neural Networks. Recurrent Neural Networks (RNNs) have been suggested to replace\n",
      "Transformer and attention due to their linearity in the context length. RNNs with Deep Linear\n",
      "Recurrent Units (LRUs) showed promising results for language modeling (Orvieto et al., 2023; De\n",
      "et al., 2024), as did Hierarchically Gated Linear RNN (HGRN) (Qin et al., 2023) and HGRN2 (Qin\n",
      "et al., 2024). A well-known RNN approach to large language modeling is RWKV (Peng et al., 2023,\n",
      "2024), showcasing competitive performance to Transformers.\n",
      "Gating. One of the key ideas of LSTM is gating, which was rediscovered and reinterpreted in many\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 8 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "Test of xLSTM’s Long Context Capabilities on Long Range Arena. To assess xLSTM’s per-\n",
      "formance on long sequences and large contexts, we compare different methods on the Long Range\n",
      "Arena (Tay et al., 2021). xLSTM demonstrates consistent strong performance on all of the tasks,\n",
      "suggesting that the xLSTM architecture is remarkably efficient in handling different aspects of long\n",
      "context problems. For more details, see Appendix B.1.3.\n",
      "4.2 Method Comparison and Ablation Study\n",
      "The main question of this paper is, what can we achieve in language modeling when scaling up the\n",
      "new LSTM variants. Therefore, we train xLSTMs, Transformers, State Space Models, and other\n",
      "methods on 15B tokens from SlimPajama in an auto-regressive language modeling setting. We\n",
      "\n",
      "COMPRESSION\n",
      "Document 1 page 0 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "- \"How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?\"\n",
      "- \"Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques.\"\n",
      "- \"Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.\"\n",
      "- \"Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures.\"\n",
      "- \"Exponential gating and modified memory structures\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 2 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "- Extended Long Short-Term Memory (xLSTM)\n",
      "- introduces two main modifications to the LSTM idea of Equation (1)\n",
      "- exponential gating and novel memory structures\n",
      "- enrich the LSTM family by two members: (i) the new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing, and (ii) the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable\n",
      "- Both sLSTM and mLSTM enhance the LSTM through exponential gating\n",
      "- To enable parallelization, the mLSTM abandons memory mixing, i.e., the hidden-hidden recurrent connections\n",
      "- Both mLSTM and sLSTM can be extended to multiple memory cells\n",
      "- sLSTM features memory\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 40 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "- Adding Exponential Gating to this architecture yields the sLSTM as depicted in Figure 9, with another large performance improvement.\n",
      "- Finally, adding the best Matrix Memory variant found in Table 8 by replacing some sLSTM blocks with the mLSTM (see Figure 10) gives xLSTM[7:1] with the best performance.\n",
      "- Details on Gating Technique Ablation Study. In Table 2 (bottom), we investigate the effect of trainable and input-dependent gates for mLSTM.\n",
      "- The results show that, in contrast to other methods (Katharopoulos et al., 2020; Sun et al., 2023; Qin et al., 2023; Katsch, 2023; Yang et al., 2023; Qin et al., 2024; Peng et al., 2024), having the gates both learnable and input dependent gives the best results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 5 from /Users/ropereira/Desktop/Projects/doc_questions_rag/data/raw//xLSTM_paper.pdf:\n",
      "\n",
      "- xLSTM Architecture\n",
      "- residually stacking building blocks (Srivastava et al., 2015; He et al., 2016)\n",
      "- most commonly used pre-LayerNorm (Ba et al., 2016b) residual backbones\n",
      "- contemporary Large Language Models\n",
      "- linear computation and constant memory complexity with respect to sequence length\n",
      "- compressive xLSTM memory\n",
      "- well suited for industrial applications and implementations on the edge\n",
      "- memory of mLSTM does not require parameters\n",
      "- computationally expensive through its d×d matrix memory and d×d update\n",
      "- trade off memory capacity against computational complexity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        pretty_print_docs(search[key]['source_documents'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
