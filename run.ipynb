{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.services.file_service import load_pdf, split_document\n",
    "from src.services.vectordb_service import ChromaDB\n",
    "from src.utils.util import pretty_print_docs\n",
    "from config import TEMPLATE, MODEL, DOC_PATH, CHUNK_SIZE, CHUNK_OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('USE ONLY the following pieces of context to answer the question at the end.\\n'\n",
      " \"If you don't know the answer, just say that you don't know, NEVER MAKE UP AN \"\n",
      " 'ANSWER or \\n'\n",
      " 'give opinions out side the context.\\n'\n",
      " 'Summarize in bullet point format. Keep the answer as concise as possible.\\n'\n",
      " '{context}\\n'\n",
      " 'Question: {question}\\n'\n",
      " 'Helpful Answer:')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file VIT.pdf ...\n",
      "File load correctly. Contains 22 pages\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf(doc_path=DOC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document spliting. Chunk size 800 - Chunk overlap 400 - Strategy recursive\n",
      "Splits generated 157\n"
     ]
    }
   ],
   "source": [
    "text_splits = split_document(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, strategy='recursive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data/chroma' removed successfully.\n",
      "Creating a local embedding vector DB on directory data/chroma\n",
      "DB created successfuly. Collection count: 157 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chroma = ChromaDB(text_splits)\n",
    "vectorstore = chroma.get_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Does the xLSTM perform better than Transformers for long sequences prediction?\"\n",
    "# question = \"What are the main restrictions of this new xLSTM arquitecture?\"\n",
    "# question = \"What is the major improvement of OmniVec over his predecessor?\"\n",
    "# question = \"What are the caracteristics of OmniVec?\"\n",
    "# question = \"What is the OmniVec backbone\"\n",
    "question = \"\"\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0)\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for search_type in ['similarity']: # 'mmr' \n",
    "    s_type = {}\n",
    "    retreiver = vectorstore.as_retriever(search_type=search_type)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retreiver,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    s_type[search_type] = result\n",
    "    search_results.append(s_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "- The OmniVec backbone consists of a shared backbone network\n",
      "- It includes a modality specific encoder and task specific heads\n",
      "- The framework facilitates end-to-end training and allows for the interchange of encoders and task heads for different modalities and tasks\n"
     ]
    }
   ],
   "source": [
    "#question = \"What are the main restrictions of this new xLSTM arquitecture?\"\n",
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        print(search[key]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "- OmniVec is a unified data and task agnostic learning framework with a single backbone\n",
      "- It utilizes modalities in different domains to aid the learning process\n",
      "- OmniVec has a novel training mechanism that groups tasks and constructs mini batches by mixing inter-modality datasets\n",
      "- The framework is highly generalizable, robust, and can adapt well to seen and unseen tasks\n",
      "- OmniVec can generalize well to tasks with different data distributions\n",
      "- It has been tested on 22 datasets spanning across image, video, point cloud, depth, audio, and text\n",
      "- OmniVec achieves state of the art or close to state of the art results in various tasks and modalities\n"
     ]
    }
   ],
   "source": [
    "#question = \"What are the main restrictions of this new xLSTM arquitecture?\"\n",
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        print(search[key]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "Document 1 page 6 from data/raw/Omnivec.pdf:\n",
      "\n",
      "randomly, and OmniVec-4 follows the settings from Sec-\n",
      "tion3. Comparatively, OmniVec-1 lags behind the others.\n",
      "Both OmniVec-2 and OmniVec-3 outperform OmniVec-1\n",
      "by around 30% to 45%, showing their efﬁcacy. However,\n",
      "OmniVec-4, which combines both approaches, performs\n",
      "better, emphasizing the beneﬁts of integrating tasks and\n",
      "modalities.\n",
      "Inﬂuence of size of the modality encoder. We evalu-\n",
      "ated the impact of enlarging the base modality encoder to\n",
      "the scale of our suggested network, using modality-speciﬁc\n",
      "data. This change slightly improved performance. For\n",
      "example, on ImageNet1K, the top-1 accuracy went from\n",
      "88.5% with the base ViT [ 19] to 89.1% with the aug-\n",
      "mented ViT having a similar parameter count, while Om-\n",
      "niVec achieved 92.4%. These ﬁndings suggest that even\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 7 from data/raw/Omnivec.pdf:\n",
      "\n",
      "SSv2 Top-1 Acc 85.4 77.3 (MVD [ 85])\n",
      "ImageNet1K Top-1 Acc 92.4 91.1 (BASIC-L [ 12])\n",
      "Sun RGBD Top-1 Acc 74.6 67.2 (Omnivore [ 25])\n",
      "ModelNet40 Overall Acc 96.6 95.4 (GeomGCNN [ 74])\n",
      "Table 13. Comparison with state of the art after ﬁne tuning on\n",
      "respective training sets.\n",
      "pretrained OmniVec.\n",
      "5. Conclusion and Limitations\n",
      "Conclusion.\n",
      "We proposed OmniVec, a uniﬁed data and task agnos-\n",
      "tic learning framework with a single backbone. The main\n",
      "idea behind OmniVec is that modalities in different do-\n",
      "mains can aid learning process. Further, we also proposed\n",
      "a novel training mechanism by grouping tasks and con-\n",
      "structing mini batches by mixing inter-modality datasets.\n",
      "With experiments on 22datasets spanning across image,video, point cloud, depth, audio, text; we show that the pro-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 7 from data/raw/Omnivec.pdf:\n",
      "\n",
      "tic learning framework with a single backbone. The main\n",
      "idea behind OmniVec is that modalities in different do-\n",
      "mains can aid learning process. Further, we also proposed\n",
      "a novel training mechanism by grouping tasks and con-\n",
      "structing mini batches by mixing inter-modality datasets.\n",
      "With experiments on 22datasets spanning across image,video, point cloud, depth, audio, text; we show that the pro-\n",
      "posed framework is highly generalizable along with being\n",
      "extremely robust. It can also generalize well to seen tasks\n",
      "with different data distribution as well as can adapt to un-\n",
      "seen tasks effectively. We also studied the cross-domain\n",
      "knowledge sharing by evaluating a zero shot video-text re-\n",
      "trieval task. We achieve state of the art or close to state of\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 4 from data/raw/Omnivec.pdf:\n",
      "\n",
      "Omnivore [ 25] Image, Video, Depth map Yes No Class. - - 71.4 - 84.0 65.4 -\n",
      "V ATT [ 1] Image, Video, Audio, Text Yes Yes Class. - 39.4 - - - - -\n",
      "Perceiver IO [ 37] Modality Agnostic No No Multiple - - - - 79.0 - 77.4\n",
      "OmniVec (pretrained)Image, Video, Audio, Text,\n",
      "Depth map, Point CloudsYes Yes Multiple 48.6 44.7 80.1 84.3 88.6 71.4 83.6\n",
      "Table 2. Comparison of OmniVec framework with similar methods that w ork on multiple modalities . We compare OmniVec with\n",
      "masked pretraining with the best reported results from resp ective publications of the compared methods. Supp. Tasks an d Supp. Modalities\n",
      "indicate Supported Tasks and Supported Modalities by respe ctive networks. In Supported (Supp.) Tasks, Class. indicat es classiﬁcation.\n",
      "Training on multiple modalities and tasks. For training\n"
     ]
    }
   ],
   "source": [
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        pretty_print_docs(search[key]['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
