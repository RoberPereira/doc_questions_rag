{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.services.file_service import load_pdf, split_document\n",
    "from src.services.vectordb_service import ChromaDB\n",
    "from src.utils.util import pretty_print_docs\n",
    "from config import TEMPLATE, MODEL, DOC_PATH, CHUNK_SIZE, CHUNK_OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Use ONLY the following pieces of context to answer the question at the end.\\n'\n",
      " \"If you don't know the answer, just say that you don't know, NEVER make up an \"\n",
      " 'answer.\\n'\n",
      " 'Summarize in bullet point format. Keep the answer as concise as possible.\\n'\n",
      " '{context}\\n'\n",
      " 'Question: {question}\\n'\n",
      " 'Helpful Answer:')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file xLSTM_paper.pdf ...\n",
      "File load correctly. Contains 55 pages\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf(doc_path=DOC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document spliting. Chunk size 800 - Chunk overlap 400 - Strategy recursive\n",
      "Splits generated 364\n"
     ]
    }
   ],
   "source": [
    "text_splits = split_document(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, strategy='recursive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = ChromaDB(text_splits)\n",
    "vectorstore = chroma.get_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Does the xLSTM perform better than Transformers for long sequences prediction?\"\n",
    "question = \"What are the main restrictions of this new xLSTM arquitecture?\"\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0)\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for search_type in ['similarity']: # 'mmr' \n",
    "    s_type = {}\n",
    "    retreiver = vectorstore.as_retriever(search_type=search_type)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retreiver,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    s_type[search_type] = result\n",
    "    search_results.append(s_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "- Expensive computational load for large language experiments\n",
      "- Not fully optimized architecture or hyperparameters, especially for larger xLSTM architectures\n",
      "- Extensive optimization process needed for xLSTM to reach its full potential\n"
     ]
    }
   ],
   "source": [
    "#question = \"What are the main restrictions of this new xLSTM arquitecture?\"\n",
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        print(search[key]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "- xLSTM demonstrates consistent strong performance on all tasks in the Long Range Arena\n",
      "- xLSTM models maintain low perplexities for longer contexts compared to other methods\n",
      "- xLSTM[1:1] is the best-performing non-Transformer model\n",
      "- xLSTM models perform favorably on language modeling compared to Transformers and State Space Models\n",
      "\n",
      "Yes, the xLSTM performs better than Transformers for long sequences prediction.\n"
     ]
    }
   ],
   "source": [
    "# question = \"Does the xLSTM perform better than Transformers for long sequences prediction?\"\n",
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        print(search[key]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "Document 1 page 8 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "Test of xLSTM’s Long Context Capabilities on Long Range Arena. To assess xLSTM’s per-\n",
      "formance on long sequences and large contexts, we compare different methods on the Long Range\n",
      "Arena (Tay et al., 2021). xLSTM demonstrates consistent strong performance on all of the tasks,\n",
      "suggesting that the xLSTM architecture is remarkably efficient in handling different aspects of long\n",
      "context problems. For more details, see Appendix B.1.3.\n",
      "4.2 Method Comparison and Ablation Study\n",
      "The main question of this paper is, what can we achieve in language modeling when scaling up the\n",
      "new LSTM variants. Therefore, we train xLSTMs, Transformers, State Space Models, and other\n",
      "methods on 15B tokens from SlimPajama in an auto-regressive language modeling setting. We\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 10 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "other methods, xLSTM models maintain low perplexities for longer contexts.\n",
      "ModelSlimPajama\n",
      "(300B) ppl ↓\n",
      "at 16k\n",
      "Llama 337.83\n",
      "Mamba 14.00\n",
      "RWKV-4 13.75\n",
      "xLSTM[7:1] 8.92\n",
      "xLSTM[1:0] 9.01\n",
      "Figure 7: Sequence extrapolation in language modeling. This is a comparison of 1.3B-sized, large\n",
      "models of xLSTM, RWKV-4, Llama, and Mamba at next token prediction on the SlimPajama\n",
      "validation set after training on 300B tokens from SlimPajama. Models are trained with context length\n",
      "2048 and then tested for context lengths up to 16384. Left: Token perplexities evaluated at different\n",
      "context lengths. In contrast to other methods, xLSTM models remain at low perplexities for longer\n",
      "contexts. Right: Prediction quality when extrapolating to long context sizes in terms of validation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 34 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "RWKV-6. The latter fails to learn the task at all in some KV=256 settings. Overall xLSTM[1:1] is the\n",
      "best-performing non-Transformer model — suggesting that it provides enhanced memory capacity,\n",
      "also in long contexts.\n",
      "Figure 15 shows the extrapolation results from Experiment 3. For xLSTM[1:1], xLSTM[1:0], and\n",
      "Mamba the model performance does not change in the extrapolation setting. The RWKV models\n",
      "(especially RWKV5) degrade slightly with increasing context length. xLSTM[1:1] performs best, as\n",
      "it maintains its superior performance of Experiment 2.\n",
      "4The keys are distributed on the “evaluation part” of the sequence given a power-law distribution. This is\n",
      "motivated by similar structures in natural language text.\n",
      "35\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 13 from data/raw/xLSTM_paper.pdf:\n",
      "\n",
      "6 Conclusion\n",
      "We have partly answered our simple question: How far do we get in language modeling when scaling\n",
      "LSTM to billions of parameters? So far, we can answer: “At least as far as current technologies\n",
      "like Transformers or State Space Models”. We have enhanced LSTM to xLSTM by exponential\n",
      "gating with memory mixing and a new memory structure. xLSTM models perform favorably on\n",
      "language modeling when compared to state-of-the-art methods like Transformers and State Space\n",
      "Models. The scaling laws indicate that larger xLSTM models will be serious competitors to current\n",
      "Large Language Models that are built with the Transformer technology. xLSTM has the potential to\n",
      "considerably impact other deep learning fields like Reinforcement Learning, Time Series Prediction,\n"
     ]
    }
   ],
   "source": [
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        pretty_print_docs(search[key]['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
