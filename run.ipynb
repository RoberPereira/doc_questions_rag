{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.services.file_service import load_pdf, split_document\n",
    "from src.services.vectordb_service import ChromaDB\n",
    "from src.utils.util import pretty_print_docs\n",
    "from config import TEMPLATE, MODEL, DOC_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Use ONLY the following pieces of context to answer the question at the end.\\n'\n",
      " \"If you don't know the answer, just say that you don't know, NEVER make up an \"\n",
      " 'answer.\\n'\n",
      " 'Summarize in bullet point format. Keep the answer as concise as possible.\\n'\n",
      " '{context}\\n'\n",
      " 'Question: {question}\\n'\n",
      " 'Helpful Answer:')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file AttentionIsAllYouNeed.pdf ...\n",
      "File load correctly. Contains 15 pages\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf(doc_path=DOC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document spliting. Chunk size 800 - Chunk overlap 400 - Strategy recursive\n",
      "Splits generated 92\n",
      "Cleaning Chroma DB\n",
      "Creating a local embedding vector DB on directory data/chroma\n",
      "DB created successfuly. Collection count: 92 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_splits = split_document(pages, chunk_size=800, chunk_overlap=400, strategy='recursive')\n",
    "chroma = ChromaDB(text_splits)\n",
    "vectorstore = chroma.get_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What are the major improvments over the previous LSTM arquitecture?\"\n",
    "question = \"Why is multi head attention has n2 complexity?\"\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0)\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for search_type in ['similarity']: # 'mmr' \n",
    "    s_type = {}\n",
    "    retreiver = vectorstore.as_retriever(search_type=search_type)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retreiver,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    s_type[search_type] = result\n",
    "    search_results.append(s_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "- Multi-head attention has n^2 complexity because it involves computing attention across all pairs of positions in the sequence, resulting in a quadratic number of operations.\n",
      "- The model employs h=8 parallel attention layers, or heads, each attending to different representation subspaces at different positions.\n"
     ]
    }
   ],
   "source": [
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        print(search[key]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY\n",
      "Document 1 page 3 from data/raw/AttentionIsAllYouNeed.pdf:\n",
      "\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2 page 4 from data/raw/AttentionIsAllYouNeed.pdf:\n",
      "\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i, KWK\n",
      "i, V WV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3 page 1 from data/raw/AttentionIsAllYouNeed.pdf:\n",
      "\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4 page 1 from data/raw/AttentionIsAllYouNeed.pdf:\n",
      "\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n"
     ]
    }
   ],
   "source": [
    "for search in search_results:\n",
    "    for key in search.keys():\n",
    "        print(str.upper(key))\n",
    "        pretty_print_docs(search[key]['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
